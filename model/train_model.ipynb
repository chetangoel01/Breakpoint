{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Re-running the model training setup since kernel was reset\n",
    "\n",
    "# Install required packages\n",
    "!pip install opencv-python-headless mediapipe scikit-learn pandas joblib tqdm --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/ismailnasri20/driver-drowsiness-dataset-ddd?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.58G/2.58G [01:36<00:00, 28.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/chetan/.cache/kagglehub/datasets/ismailnasri20/driver-drowsiness-dataset-ddd/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ismailnasri20/driver-drowsiness-dataset-ddd\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751556970.365575 1618214 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1751556970.368295 1779033 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1751556970.377352 1779036 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'a' to record ALERT, 'd' for DROWSY. Need 5 of each.\n",
      "Added DROWSY sample\n",
      "Added ALERT sample\n",
      "Added ALERT sample\n",
      "Added ALERT sample\n",
      "Added ALERT sample\n",
      "Added DROWSY sample\n",
      "Added DROWSY sample\n",
      "Added DROWSY sample\n",
      "Added DROWSY sample\n",
      "Added ALERT sample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n       alert       0.76      0.87      0.81      3889\\n      drowsy       0.87      0.76      0.81      4469\\n\\n    accuracy                           0.81      8358\\n   macro avg       0.81      0.81      0.81      8358\\nweighted avg       0.82      0.81      0.81      8358\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This script modifies the training pipeline to include:\n",
    "# - Manual capture of 5 'alert' and 5 'drowsy' baseline samples from webcam\n",
    "# - Uses these samples to personalize and extend the training dataset\n",
    "# - Computes a personalized EAR threshold for PERCLOS\n",
    "# - Trains a RandomForest model on all features\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset with EAR, head pitch, PERCLOS columns\n",
    "df = pd.read_csv(\"drowsiness_augmented_dataset.csv\")\n",
    "baseline_samples = []\n",
    "\n",
    "# Setup MediaPipe\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "LEFT_EYE = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE = [362, 387, 385, 263, 373, 380]\n",
    "CHIN = 152\n",
    "FOREHEAD = 10\n",
    "\n",
    "def calculate_ear(landmarks, indices):\n",
    "    pts = np.array([[landmarks[i].x, landmarks[i].y] for i in indices])\n",
    "    v1 = np.linalg.norm(pts[1] - pts[5])\n",
    "    v2 = np.linalg.norm(pts[2] - pts[4])\n",
    "    h = np.linalg.norm(pts[0] - pts[3])\n",
    "    return (v1 + v2) / (2.0 * h) if h > 0 else 0\n",
    "\n",
    "def estimate_head_pitch(landmarks):\n",
    "    chin = np.array([landmarks[CHIN].x, landmarks[CHIN].y])\n",
    "    forehead = np.array([landmarks[FOREHEAD].x, landmarks[FOREHEAD].y])\n",
    "    delta = chin - forehead\n",
    "    return np.arctan2(delta[1], delta[0])\n",
    "\n",
    "# Setup camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "ear_history = deque(maxlen=5)\n",
    "print(\"Press 'a' to record ALERT, 'd' for DROWSY. Need 5 of each.\")\n",
    "\n",
    "while len([x for x in baseline_samples if x[1] == 'alert']) < 5 or len([x for x in baseline_samples if x[1] == 'drowsy']) < 5:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = face_mesh.process(rgb)\n",
    "\n",
    "    if result.multi_face_landmarks:\n",
    "        landmarks = result.multi_face_landmarks[0].landmark\n",
    "        left_ear = calculate_ear(landmarks, LEFT_EYE)\n",
    "        right_ear = calculate_ear(landmarks, RIGHT_EYE)\n",
    "        avg_ear = (left_ear + right_ear) / 2.0\n",
    "        head_pitch = estimate_head_pitch(landmarks)\n",
    "\n",
    "        ear_history.append(avg_ear)\n",
    "\n",
    "        if len(ear_history) == ear_history.maxlen:\n",
    "            ear_std = np.std(ear_history)\n",
    "            perclos = sum(1 for e in ear_history if e < 0.21) / len(ear_history)\n",
    "\n",
    "            feature_vector = [left_ear, right_ear, avg_ear, head_pitch, ear_std, perclos]\n",
    "\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('a') and len([x for x in baseline_samples if x[1] == 'alert']) < 5:\n",
    "                baseline_samples.append((feature_vector, 'alert'))\n",
    "                print(\"Added ALERT sample\")\n",
    "            elif key == ord('d') and len([x for x in baseline_samples if x[1] == 'drowsy']) < 5:\n",
    "                baseline_samples.append((feature_vector, 'drowsy'))\n",
    "                print(\"Added DROWSY sample\")\n",
    "\n",
    "        label_text = f\"EAR: {avg_ear:.3f}\"\n",
    "        cv2.putText(frame, label_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "    else:\n",
    "        cv2.putText(frame, \"No face\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (128, 128, 128), 2)\n",
    "\n",
    "    cv2.imshow(\"Capture Baseline\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Append baseline samples to dataset\n",
    "baseline_df = pd.DataFrame([x[0] + [x[1]] for x in baseline_samples], columns=df.columns)\n",
    "df = pd.concat([df, baseline_df], ignore_index=True)\n",
    "df.to_csv(\"drowsiness_augmented_dataset_with_baseline.csv\", index=False)\n",
    "\n",
    "# Train model with added baseline data\n",
    "X = df.drop(\"label\", axis=1).values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "\n",
    "# Save updated model\n",
    "joblib.dump(model, \"rf_drowsiness_model.pkl\")\n",
    "joblib.dump(scaler, \"rf_scaler.pkl\")\n",
    "joblib.dump(le, \"rf_label_encoder.pkl\")\n",
    "\n",
    "report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751557367.040957 1618214 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1751557367.053100 1791830 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1751557367.062601 1791830 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real-time drowsiness detection with fixed confidence threshold (20%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     94\u001b[39m     cv2.putText(frame, status_text, (\u001b[32m10\u001b[39m, \u001b[32m30\u001b[39m), cv2.FONT_HERSHEY_SIMPLEX, \u001b[32m0.8\u001b[39m, box_color, \u001b[32m2\u001b[39m)\n\u001b[32m     95\u001b[39m     cv2.imshow(\u001b[33m\"\u001b[39m\u001b[33mDrowsiness Detection\u001b[39m\u001b[33m\"\u001b[39m, frame)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m & \u001b[32m0xFF\u001b[39m == \u001b[32m27\u001b[39m:  \u001b[38;5;66;03m# ESC\u001b[39;00m\n\u001b[32m     98\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    100\u001b[39m cap.release()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "\n",
    "# Load model, scaler, and encoder\n",
    "model = joblib.load(\"rf_drowsiness_model.pkl\")\n",
    "scaler = joblib.load(\"rf_scaler.pkl\")\n",
    "encoder = joblib.load(\"rf_label_encoder.pkl\")\n",
    "\n",
    "# MediaPipe setup\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Facial landmark indices\n",
    "LEFT_EYE = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE = [362, 387, 385, 263, 373, 380]\n",
    "CHIN = 152\n",
    "FOREHEAD = 10\n",
    "\n",
    "def calculate_ear(landmarks, indices):\n",
    "    pts = np.array([[landmarks[i].x, landmarks[i].y] for i in indices])\n",
    "    v1 = np.linalg.norm(pts[1] - pts[5])\n",
    "    v2 = np.linalg.norm(pts[2] - pts[4])\n",
    "    h = np.linalg.norm(pts[0] - pts[3])\n",
    "    return (v1 + v2) / (2.0 * h) if h > 0 else 0\n",
    "\n",
    "def estimate_head_pitch(landmarks):\n",
    "    chin = np.array([landmarks[CHIN].x, landmarks[CHIN].y])\n",
    "    forehead = np.array([landmarks[FOREHEAD].x, landmarks[FOREHEAD].y])\n",
    "    delta = chin - forehead\n",
    "    return np.arctan2(delta[1], delta[0])  # vertical over horizontal\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "ear_history = deque(maxlen=5)\n",
    "\n",
    "print(\"Real-time drowsiness detection with fixed confidence threshold (20%)\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    threshold = 0.20  # fixed 20% confidence threshold\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = face_mesh.process(rgb)\n",
    "\n",
    "    status_text = \"No Face\"\n",
    "    box_color = (128, 128, 128)\n",
    "\n",
    "    if result.multi_face_landmarks:\n",
    "        landmarks = result.multi_face_landmarks[0].landmark\n",
    "\n",
    "        left_ear = calculate_ear(landmarks, LEFT_EYE)\n",
    "        right_ear = calculate_ear(landmarks, RIGHT_EYE)\n",
    "        avg_ear = (left_ear + right_ear) / 2.0\n",
    "        ear_history.append(avg_ear)\n",
    "\n",
    "        head_pitch = estimate_head_pitch(landmarks)\n",
    "\n",
    "        # Temporal features\n",
    "        if len(ear_history) == ear_history.maxlen:\n",
    "            ear_std = np.std(ear_history)\n",
    "            perclos = sum(1 for e in ear_history if e < 0.21) / len(ear_history)\n",
    "        else:\n",
    "            ear_std = 0\n",
    "            perclos = 0\n",
    "\n",
    "        # Final feature vector\n",
    "        features = np.array([[left_ear, right_ear, avg_ear, head_pitch, ear_std, perclos]])\n",
    "        features_scaled = scaler.transform(features)\n",
    "        probs = model.predict_proba(features_scaled)[0]\n",
    "        predicted_index = np.argmax(probs)\n",
    "        confidence = probs[predicted_index]\n",
    "        label = encoder.inverse_transform([predicted_index])[0]\n",
    "\n",
    "        # Apply threshold\n",
    "        if confidence >= threshold:\n",
    "            status_text = f\"{label} ({confidence:.2f})\"\n",
    "            box_color = (0, 255, 0) if label == \"alert\" else (0, 0, 255)\n",
    "        else:\n",
    "            status_text = f\"Uncertain ({confidence:.2f})\"\n",
    "            box_color = (0, 255, 255)\n",
    "\n",
    "    # Draw result on frame\n",
    "    cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, box_color, 2)\n",
    "    cv2.imshow(\"Drowsiness Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
